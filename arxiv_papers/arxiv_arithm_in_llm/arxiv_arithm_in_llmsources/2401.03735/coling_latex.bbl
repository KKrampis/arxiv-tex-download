\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Alain and Bengio(2016)}]{alain2016understanding}
Guillaume Alain and Yoshua Bengio. 2016.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock \emph{arXiv preprint arXiv:1610.01644}.

\bibitem[{Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen et~al.}]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al. 2023.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}.

\bibitem[{Belinkov(2022)}]{belinkov2022probing}
Yonatan Belinkov. 2022.
\newblock Probing classifiers: Promises, shortcomings, and advances.
\newblock \emph{Computational Linguistics}, 48(1):207--219.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:1877--1901.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Dziri et~al.(2024)Dziri, Lu, Sclar, Li, Jiang, Lin, Welleck, West, Bhagavatula, Le~Bras et~al.}]{dziri2024faith}
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang~Lorraine Li, Liwei Jiang, Bill~Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le~Bras, et~al. 2024.
\newblock Faith and fate: Limits of transformers on compositionality.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Engels et~al.(2024)Engels, Liao, Michaud, Gurnee, and Tegmark}]{engels2024not}
Joshua Engels, Isaac Liao, Eric~J Michaud, Wes Gurnee, and Max Tegmark. 2024.
\newblock Not all language model features are linear.
\newblock \emph{arXiv preprint arXiv:2405.14860}.

\bibitem[{Golkar et~al.(2023)Golkar, Pettee, Eickenberg, Bietti, Cranmer, Krawezik, Lanusse, McCabe, Ohana, Parker et~al.}]{golkar2023xval}
Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et~al. 2023.
\newblock xval: A continuous number encoding for large language models.
\newblock In \emph{NeurIPS 2023 AI for Science Workshop}.

\bibitem[{Gurnee and Tegmark(2023)}]{gurnee2023language}
Wes Gurnee and Max Tegmark. 2023.
\newblock Language models represent space and time.
\newblock \emph{arXiv preprint arXiv:2310.02207}.

\bibitem[{Hanna et~al.(2024)Hanna, Liu, and Variengien}]{hanna2024does}
Michael Hanna, Ollie Liu, and Alexandre Variengien. 2024.
\newblock How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}.

\bibitem[{Li et~al.(2021)Li, Nye, and Andreas}]{li2021implicit}
Belinda~Z Li, Maxwell Nye, and Jacob Andreas. 2021.
\newblock Implicit representations of meaning in neural language models.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 1813--1827.

\bibitem[{Li et~al.(2022)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and Wattenberg}]{li2022emergent}
Kenneth Li, Aspen~K Hopkins, David Bau, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin Wattenberg. 2022.
\newblock Emergent world representations: Exploring a sequence model trained on a synthetic task.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim et~al.}]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al. 2023{\natexlab{a}}.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Zhao, Chia, Ding, Bing, Joty, and Poria}]{li2023chain}
Xingxuan Li, Ruochen Zhao, Yew~Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya Poria. 2023{\natexlab{b}}.
\newblock Chain of knowledge: A framework for grounding large language models with structured knowledge bases.
\newblock \emph{arXiv preprint arXiv:2305.13269}.

\bibitem[{McGrath et~al.(2023)McGrath, Rahtz, Kramar, Mikulik, and Legg}]{mcgrath2023hydra}
Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. 2023.
\newblock The hydra effect: Emergent self-repair in language model computations.
\newblock \emph{arXiv preprint arXiv:2307.15771}.

\bibitem[{Muller et~al.(2018)Muller, Brisebarre, De~Dinechin, Jeannerod, Lefevre, Melquiond, Revol, Stehl{\'e}, Torres et~al.}]{muller2018handbook}
Jean-Michel Muller, Nicolas Brisebarre, Florent De~Dinechin, Claude-Pierre Jeannerod, Vincent Lefevre, Guillaume Melquiond, Nathalie Revol, Damien Stehl{\'e}, Serge Torres, et~al. 2018.
\newblock \emph{Handbook of floating-point arithmetic}.
\newblock Springer.

\bibitem[{Nanda et~al.(2023)Nanda, Lee, and Wattenberg}]{nanda2023emergent}
Neel Nanda, Andrew Lee, and Martin Wattenberg. 2023.
\newblock Emergent linear representations in world models of self-supervised sequence models.
\newblock In \emph{Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP}, pages 16--30.

\bibitem[{Nijkamp et~al.(2022)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese, and Xiong}]{nijkamp2022codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.
\newblock Codegen: An open large language model for code with multi-turn program synthesis.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock Gpt-4 technical report.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:27730--27744.

\bibitem[{Shi et~al.(2016)Shi, Padhi, and Knight}]{shi2016does}
Xing Shi, Inkit Padhi, and Kevin Knight. 2016.
\newblock Does string-based neural mt learn source syntax?
\newblock In \emph{Proceedings of the 2016 conference on empirical methods in natural language processing}, pages 1526--1534.

\bibitem[{Stolfo et~al.(2023)Stolfo, Belinkov, and Sachan}]{stolfo2023mechanistic}
Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. 2023.
\newblock A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 7035--7052.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto}]{taori2023stanford}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following llama model.

\bibitem[{Tenney et~al.(2018)Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Van~Durme, Bowman, Das et~al.}]{tenney2018you}
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R~Thomas McCoy, Najoung Kim, Benjamin Van~Durme, Samuel~R Bowman, Dipanjan Das, et~al. 2018.
\newblock What do you learn from context? probing for sentence structure in contextualized word representations.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Wallace et~al.(2019)Wallace, Wang, Li, Singh, and Gardner}]{wallace2019nlp}
Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019.
\newblock Do nlp models know numbers? probing numeracy in embeddings.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 5307--5315.

\bibitem[{Wang et~al.(2023)Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and Sui}]{wang2023math}
Peiyi Wang, Lei Li, Zhihong Shao, RX~Xu, Damai Dai, Yifei Li, Deli Chen, Y~Wu, and Zhifang Sui. 2023.
\newblock Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2312.08935}.

\bibitem[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou et~al.}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al. 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:24824--24837.

\bibitem[{Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu}]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}.

\bibitem[{Zhao et~al.(2023)Zhao, Li, Joty, Qin, and Bing}]{zhao2023verify}
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023.
\newblock Verify-and-edit: A knowledge-enhanced chain-of-thought framework.
\newblock \emph{arXiv preprint arXiv:2305.03268}.

\bibitem[{Zhong et~al.(2024)Zhong, Liu, Tegmark, and Andreas}]{zhong2024clock}
Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. 2024.
\newblock The clock and the pizza: Two stories in mechanistic explanation of neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\end{thebibliography}
